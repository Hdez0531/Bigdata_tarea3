# 1️Inicia ZooKeeper
sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties &

# Espera unos segundos y presiona Enter para volver al prompt.

# 2️Inicia Kafka
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &

# Espera unos segundos y presiona Enter para volver al prompt.

# 3️Crea el topic "sensor_data"
sudo /opt/Kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sensor_data

# 4️Ejecuta el productor en una terminal
python3 kafka_producer.py

# 5 En otra terminal (no cierres la anterior), ejecuta el consumidor con Spark Streaming
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 spark_streaming_consumer.py

Cuando ejecutes el consumidor, verás que Spark empieza a procesar los datos que el productor está enviando cada dos segundos.
Y los resultados aparecerán en tu consola y en el archivo CSV que configuraste en:
/home/bigdatavm/spark_output_promedios


